{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import clean_dataset\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Turn interactive plotting off\n",
    "plt.ioff()\n",
    "\n",
    "clean_dataset.clean_unsupervised()\n",
    "# read input text and put data inside a data frame\n",
    "data = pd.read_csv(\"../data/base_prospect_unsuppervised.csv\",encoding=\"ISO-8859-1\")\n",
    "# prospect =  pd.DataFrame(prospect)\n",
    "data['risque'] = data['risque'].astype(object)\n",
    "data['ca_total_FL'] = data['ca_total_FL'].astype(object)\n",
    "data['effectif'] = data['effectif'].astype(object)\n",
    "\n",
    "data.head\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "feature_names  = data.columns.values[2:-1]\n",
    "X = data[feature_names]\n",
    "y = data[\"code_cr\"]\n",
    "#La liste des caisses régionales\n",
    "lst_caisse=data['code_cr'].unique()\n",
    "\n",
    "X_cat = X.select_dtypes(exclude=['float64','int64'])\n",
    "\n",
    "# Disjonction with OneHotEncoder\n",
    "# encoder.fit(X_cat)\n",
    "# X_cat = encoder.transform(X_cat).toarray()\n",
    "\n",
    "X_cat = pd.get_dummies(X_cat)\n",
    "columns_cat = X_cat.columns\n",
    "columns_cat\n",
    "\n",
    "# X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traitement des variable binaire et ordinale separement\n",
    "# ordi_col = [\"ca_total_FL\"]\n",
    "\n",
    "# X_ordinal = X[ordi_col]\n",
    "# X_binaire = X.select_dtypes(exclude=['float64','int64']).drop(columns=ordi_col)\n",
    "\n",
    "# encoder.fit(X_binaire)\n",
    "# X_binaire = encoder.transform(X_binaire).toarray()\n",
    "\n",
    "# X_cat = pd.concat([pd.DataFrame(X_binaire), X_ordinal], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 3 - Cluster\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "# TODO\n",
    "X_num = X.select_dtypes(include=['float64','int64']).drop(columns=['chgt_dir'])\n",
    "x_columns = X_num.columns\n",
    "\n",
    "X_num_norm = scaler.fit_transform(X_num)\n",
    "X_num_norm = pd.DataFrame(X_num_norm, columns=x_columns)\n",
    "print(X_num_norm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_num_cat = pd.concat([X_num_norm, X_cat], axis=1).columns\n",
    "X_num_cat = pd.concat([pd.DataFrame(X_num_norm), X_cat], axis=1).to_numpy()\n",
    "\n",
    "\n",
    "print(pd.DataFrame(columns_num_cat))\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "print(X.columns)\n",
    "X_num_cat\n",
    "\n",
    "\n",
    "matrice_corr = X.corr()\n",
    "\n",
    "print(\"La matrice de corrélation est : \\n\", matrice_corr)\n",
    "\n",
    "\n",
    "# Compute R-square, i.e. V_inter/V\n",
    "from R_square_clustering import r_square\n",
    "from purity import purity_score\n",
    "\n",
    "# Plot elbow graphs for KMeans using R square and purity scores\n",
    "lst_k=range(2,20)\n",
    "lst_rsq = []\n",
    "lst_purity = []\n",
    "for k in lst_k:\n",
    "    est=KMeans(n_clusters=k)\n",
    "    est.fit(X_num_cat)\n",
    "    lst_rsq.append(r_square(X_num_cat, est.cluster_centers_,est.labels_,k))\n",
    "    # TODO: complete lst_purity\n",
    "    \n",
    "print(lst_purity, lst_rsq)\n",
    "fig = plt.figure()\n",
    "plt.plot(lst_k, lst_rsq, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('RSQ')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.savefig('fig/k-means_elbow_method')\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def bench_k_means(kmeans, name, data, labels):\n",
    "    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kmeans : KMeans instance\n",
    "        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n",
    "        already set.\n",
    "    name : str\n",
    "        Name given to the strategy. It will be used to show the results in a\n",
    "        table.\n",
    "    data : ndarray of shape (n_samples, n_features)\n",
    "        The data to cluster.\n",
    "    labels : ndarray of shape (n_samples,)\n",
    "        The labels used to compute the clustering metrics which requires some\n",
    "        supervision.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
    "    fit_time = time() - t0\n",
    "    results = [name, fit_time, estimator[-1].inertia_]\n",
    "\n",
    "    # Define the metrics which require only the true labels and estimator\n",
    "    # labels\n",
    "    clustering_metrics = [\n",
    "        metrics.homogeneity_score,\n",
    "        metrics.completeness_score,\n",
    "        metrics.v_measure_score,\n",
    "        metrics.adjusted_rand_score,\n",
    "        metrics.adjusted_mutual_info_score,\n",
    "    ]\n",
    "    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n",
    "\n",
    "    # The silhouette score requires the full dataset\n",
    "    results += [\n",
    "        metrics.silhouette_score(\n",
    "            data,\n",
    "            estimator[-1].labels_,\n",
    "            metric=\"euclidean\",\n",
    "            sample_size=300,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Show the results\n",
    "    formatter_result = (\n",
    "        \"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n",
    "    )\n",
    "    print(formatter_result.format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(82 * \"_\")\n",
    "print(\"init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette\")\n",
    "n_cluster = 5\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=n_cluster, n_init=15, random_state=0)\n",
    "labels = kmeans.fit(X_num_cat).labels_\n",
    "\n",
    "bench_k_means(kmeans=kmeans, name=\"k-means++\", data=X_num_cat, labels=labels)\n",
    "\n",
    "# pca = PCA(n_components=5).fit(X_num_cat)\n",
    "# kmeans = KMeans(init=pca.components_, n_clusters=5, n_init=20)\n",
    "# labels = kmeans.fit(X_num_cat).labels_\n",
    "\n",
    "# bench_k_means(kmeans=kmeans, name=\"PCA-based\", data=X_num_cat, labels=labels)\n",
    "\n",
    "k_centroids = kmeans.cluster_centers_\n",
    "\n",
    "# print(kmeans.cluster_centers_)\n",
    "for index, centroid in enumerate(k_centroids):\n",
    "    profiles = pd.DataFrame([centroid], columns=columns_num_cat).transpose().reset_index()\n",
    "    # Rename the index column to the original column name\n",
    "    # print(centroid,index)\n",
    "    profiles.to_csv(\"../data/profile_{}.csv\".format(index), index=False)\n",
    "\n",
    "profiles = pd.DataFrame(k_centroids, columns=columns_num_cat).transpose().reset_index()\n",
    "col = [\"Profile_{}\".format(i) for i in range(n_cluster)]\n",
    "profiles_col = [\"features\"]\n",
    "profiles_col.extend(col)\n",
    "profiles.columns=profiles_col\n",
    "# Rename the index column to the original column name\n",
    "profiles.to_csv(\"../data/profile.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_clusters_mix = 60\n",
    "kmeans_mixed = KMeans(init=\"k-means++\", n_clusters=n_clusters_mix, n_init=20, random_state=0)\n",
    "labels_mix = kmeans.fit(X_num_cat).labels_\n",
    "\n",
    "bench_k_means(kmeans=kmeans_mixed, name=\"k-means++\", data=X_num_cat, labels=labels)\n",
    "\n",
    "k_centroids = kmeans_mixed.cluster_centers_\n",
    "#hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "lst_labels  = [i for i in range(0, n_clusters_mix)]\n",
    "\n",
    "linkage_matrix = linkage(k_centroids, 'ward')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=lst_labels\n",
    ")\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward)')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig/hierarchical-clustering')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "n_cluster_cah = 9\n",
    "print(\"Compute unstructured hierarchical clustering...\")\n",
    "ward = AgglomerativeClustering(n_clusters=n_cluster_cah, linkage=\"ward\").fit(k_centroids)\n",
    "labels = ward.labels_\n",
    "print(f\"Number of points: {labels.size}\")\n",
    "\n",
    "\n",
    "# silhouettes= metrics.silhouette_samples(k_centroids, labels)\n",
    "silhouette_avg = metrics.silhouette_score(k_centroids, labels)\n",
    "davies_bouldin_score=metrics.davies_bouldin_score(k_centroids, labels)\n",
    "print(\"test\",metrics.calinski_harabasz_score(k_centroids, labels))\n",
    "\n",
    "# print(\"Les indices de silhouettes :\", silhouettes)\n",
    "\n",
    "print(\"L'indice de silhouette moyen est :\", silhouette_avg)\n",
    "print(\"L'indice de davies_bouldin_score moyen est :\", davies_bouldin_score)\n",
    "ward.children_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, fcluster\n",
    "\n",
    "\n",
    "classes =fcluster(linkage_matrix, t=n_cluster_cah, criterion='maxclust')\n",
    "classes\n",
    "classes_indice = []\n",
    "for i in range(1,n_cluster_cah+1):\n",
    "    classes_indice.append(k_centroids[[j for j, val in enumerate(classes) if val == i]])\n",
    "profil_cah = pd.DataFrame(classes_indice[0], columns=columns_num_cat).mean(axis=0)\n",
    "arr = []\n",
    "for i in range(0,n_cluster_cah):\n",
    "    # print(classes_indice[i])\n",
    "    arr.append(pd.DataFrame(classes_indice[i]).mean(axis=0).values)\n",
    "    \n",
    "profile_cah = pd.DataFrame(arr,columns=columns_num_cat).transpose().reset_index()\n",
    "profile_cah.to_csv(\"../data/profile_cah.csv\", index=False)\n",
    "\n",
    "\n",
    "    # profil_cah = pd.concat([profil_cah,pd.DataFrame(classes_indice[i], columns=columns_num_cat).mean(axis=0)])\n",
    "profil_cah\n",
    "classes_indice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "274cf1cd44d75c296ad468b847379e30b7fbc6e3132e4e5e37d60078b9b24cbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
